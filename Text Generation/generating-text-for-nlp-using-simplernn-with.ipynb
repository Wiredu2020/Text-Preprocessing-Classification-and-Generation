{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text for NLP using SimpleRNN with keras\n",
    "\n",
    "### Table of interest:\n",
    "1. Introduction.\n",
    "2. Import data and preprocessing.\n",
    "3. Model Building.\n",
    "4. Model Training and Prediction.\n",
    "5. Conclusion.\n",
    "\n",
    "This exemple was simply taken in the book **Deep Learning with Keras**, *by Antonio Gulli and Sujit Pal, 2017.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction.\n",
    "\n",
    "RNNs have been used extensively by the **natural language processing (NLP)** community for various applications. One such application is building language models. A language model allows us to predict the probability of a word in a text given the previous words. Language models are important for various higher level tasks such as machine translation, spelling correction, and so on.\n",
    "\n",
    "A side effect of the ability to predict the next word given previous words is a generative model that allows us to generate text by ***sampling from the output probabilities***. In language modeling, our input is typically a sequence of words and the output is a sequence of predicted words. The training data used is existing unlabeled text, where we set the label **y(t)** at time **t** to be the input **x(t+1)** at time **t+1**.\n",
    "\n",
    "We will train a character based language model on the text of ***Alice in Wonderland*** to predict the next character given **10** previous characters. We have chosen to build a character-based model here because it has a smaller vocabulary and trains quicker. The idea is the same as using a **word-based language model**, except we use characters instead of words. We will then use the trained model to generate some text in the same style.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/alice_in_wonderland.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import data and preprocessing.\n",
    "\n",
    "We read our input text from the text of ***Alice in Wonderland*** on the Project Gutenberg website (https://www.gutenberg.org/files/11/11.txt). The file contains line breaks and non-ASCII characters, so we do some preliminary cleanup and write out the contents into a variable called text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(\"../input/alice_in_wonderland.txt\", 'rb')\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line = line.strip().lower()\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of characters that occur in the text\n",
    "chars = set([c for c in text])\n",
    "# Total items in our vocabulary\n",
    "nb_chars = len(chars)\n",
    "# lookup tables to deal with indexes of characters rather than the characters themselves.\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the input and label texts. We do this by stepping through the text by a numberof characters given by the `STEP` variable and then extracting a span of text whose size is determined by the `SEQLEN` variable. The next character after the span is our label character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to vectorize these input and label texts. Each row of the input to the RNN\n",
    "corresponds to one of the input texts shown previously. There are `SEQLEN` characters in this input, and since our vocabulary size is given by `nb_chars`, we represent each input character as a **one-hot encoded vector** of size (`nb_chars`). Thus each input row is a tensor of size (`SEQLEN` and `nb_chars`). \n",
    "\n",
    "Our output label is a single character, so similar to the way we represent each character of our input, it is represented as a **one-hot vector of size** (`nb_chars`). Thus, the shape of each label is `nb_chars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building.\n",
    "\n",
    "* We define the RNN's output dimension to have a size of **128**. This is a hyper-parameter that needs to be determined by experimentation. In general, if we choose too small a size, then the model does not have sufficient capacity for generating good text, and you will see long runs of repeating characters or runs of repeating word groups. On the other hand, if the value chosen is too large, the model has too many parameters and needs a lot more data to train effectively.\n",
    "* We want to return a single character as output, not a sequence of characters, so `return_sequences=False`.\n",
    "* In addition, we set `unroll=True` because it improves performance on the TensorFlow backend.\n",
    "* The RNN is connected to a dense (fully connected) layer. The dense layer has (nb_char) units, which emits scores for each of the characters in the vocabulary. The activation on the dense layer is a softmax, which normalizes the scores to probabilities. The character with the highest probability is chosen as the prediction.\n",
    "* We compile the model with the categorical cross-entropy loss function, a good loss function for categorical outputs, and the RMSprop optimizer.\n",
    "\n",
    "Now let's review the code of model building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False, \n",
    "                    input_shape=(SEQLEN, nb_chars), \n",
    "                    unroll=True))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Prediction\n",
    "* Our training approach is a little different from what we have seen so far. So far our approach has been to train a model for a fixed number of epochs, then evaluate it against a portion of held-out test data. Since we don't have any labeled data here, we train the model for an epoch(`NUM_EPOCHS_PER_ITERATION=1`) then test it. We continue training like this for **25** (`NUM_ITERATIONS=25`) iterations, stopping once we see intelligible output. So effectively, we are training for NUM_ITERATIONS epochs and testing the model after each epoch.\n",
    "\n",
    "* Our test consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from our previous run, and generating another character from the model. We continue this 100 times (`NUM_PREDS_PER_EPOCH=100`) and generate and print the resulting string. The string gives us an indication of the quality of the model.\n",
    "\n",
    "Let's review the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 6s 38us/step - loss: 2.3461\n",
      "\n",
      "Generating from seed:  for insta\n",
      " for instan the hare the hare the hare the hare the hare the hare the hare the hare the hare the hare the hare==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 2.0212\n",
      "\n",
      "Generating from seed: guinea-pig\n",
      "guinea-pige the moule sad the mouthe she has the maste the mouthe she has the maste the mouthe she has the mas==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.9137\n",
      "\n",
      "Generating from seed: lse have y\n",
      "lse have you dound the saide the saide the saide the saide the saide the saide the saide the saide the saide t==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.8344\n",
      "\n",
      "Generating from seed:  `or would\n",
      " `or would be them the made the mouse the rabbet was and then the rabbet was and then the rabbet was and then ==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.7682\n",
      "\n",
      "Generating from seed: ed eyes, a\n",
      "ed eyes, and the that she was the was the was the was the was the was the was the was the was the was the was ==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.7139\n",
      "\n",
      "Generating from seed: ghed the h\n",
      "ghed the had begand the said the mouse the round be a little grown at the mouse the round be a little grown at==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 35us/step - loss: 1.6674\n",
      "\n",
      "Generating from seed: nd she squ\n",
      "nd she squeen the dont of the came of the came of the came of the came of the came of the came of the came of ==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 6s 43us/step - loss: 1.6285\n",
      "\n",
      "Generating from seed: `\"we know \n",
      "`\"we know it was a little she was no said the mouse say and were was a little she was no said the mouse say an==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 6s 39us/step - loss: 1.5963\n",
      "\n",
      "Generating from seed:  shouldn't\n",
      " shouldn't the didden the dore the dore the dore the dore the dore the dore the dore the dore the dore the dor==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 37us/step - loss: 1.5679\n",
      "\n",
      "Generating from seed: e, why, i \n",
      "e, why, i don't the sarden it was sor her her her her her her her her her her her her her her her her her her ==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 37us/step - loss: 1.5430\n",
      "\n",
      "Generating from seed:  further o\n",
      " further off the was a little grided the don't be and grown the court of it was and grown the court of it was ==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.5207\n",
      "\n",
      "Generating from seed:  hand and \n",
      " hand and began the same think you know in a little grow her the sood a little grow her the sood a little grow==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.5002\n",
      "\n",
      "Generating from seed:  thought, \n",
      " thought, `i had not and the white rabbit she was not of the was a look of the white rabbit she was not of the==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.4830\n",
      "\n",
      "Generating from seed:  moment!' \n",
      " moment!' `i should catly the march hare and the march hare and the march hare and the march hare and the marc==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.4654\n",
      "\n",
      "Generating from seed: derland of\n",
      "derland of the turples the way to she was not the queen the queen the queen the queen the queen the queen the ==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.4511\n",
      "\n",
      "Generating from seed: door, so s\n",
      "door, so she could not to the same the stien you might alice was not the reat her four the court off other she==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.4380\n",
      "\n",
      "Generating from seed: g, and thi\n",
      "g, and thing to do and soon a mock turtle alice as it was to be a very such a sarden in a long to the thing to==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.4242\n",
      "\n",
      "Generating from seed:  and down \n",
      " and down again, and the mock turtle the caterpillar the white rabbit was an the white rabbit was an the white==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.4137\n",
      "\n",
      "Generating from seed: r, she beg\n",
      "r, she began the court of the time the gryphon a grides the court of the time the gryphon a grides the court o==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.4020\n",
      "\n",
      "Generating from seed:  tone, and\n",
      " tone, and she said to herself a little she was so ond of the same so play to see the pate of the soon of the ==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.3925\n",
      "\n",
      "Generating from seed: ce had not\n",
      "ce had not to herself the partone the patter.  `it made a preading to herself the partone the patter.  `it mad==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.3830\n",
      "\n",
      "Generating from seed: she put th\n",
      "she put the mock turtle she was she was all the caterpillar the caterpillar the caterpillar the caterpillar th==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.3751\n",
      "\n",
      "Generating from seed: heads belo\n",
      "heads beloner the dormouse she had not to go be never head and the patter what a same again, and was not the m==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.3673\n",
      "\n",
      "Generating from seed: away the m\n",
      "away the mock turtle said to herself `the offeed the queen went on again, and began the other she could not to==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "143504/143504 [==============================] - 5s 36us/step - loss: 1.3604\n",
      "\n",
      "Generating from seed: e comes, t\n",
      "e comes, the partone to the same again, and all the same again, and all the same again, and all the same again\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"\\nGenerating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by the end of the 25th epoch, it has learned to spell reasonably well, although it has trouble expressing coherent thoughts. The amazing thing about this model is that it is character-based and has no knowledge of words, yet it learns to spell words that look like they might have come from the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion.\n",
    "\n",
    "Generating the next character or next word of text is not the only thing you can do with this sort of model. This kind of model has been successfully used to make **stock predictions** (for more information refer to the article: ***Financial Market Time Series Prediction with Recurrent Neural Networks, by A. Bernal, S. Fok, and R. Pidaparthi, 2012***) and generate classical music (for more information refer to the article: ***DeepBach: A Steerable Model for Bach Chorales Generation, by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016***), to name a few interesting applications. **Andrej Karpathy** covers a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his blog post at: ***The Unreasonable Effectiveness of Recurrent Neural Networks at http://karpathy.github.io/2015/05/21/rnn-effectiveness/.***\n",
    "\n",
    "### References:\n",
    "* **Deep Learning with Keras**, *by Antonio Gulli and Sujit Pal, 2017.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hope that you find this notebook helpful. More to come.**\n",
    "\n",
    "**Please upvote this, to keep me motivate for doing better.**\n",
    "\n",
    "**Thanks.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
